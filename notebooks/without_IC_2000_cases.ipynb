{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from math import sqrt\n",
    "import scipy.stats as sts\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from supplement_package import game\n",
    "from supplement_package import variables_pecan\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supplement_package.game.stackelberg import StackelbergPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "\n",
    "from supplement_package.gurobi_implementation.gurobi import GurobiSolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preliminaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_keys = [661, 1642, 2335, 2361, 2818, 3039, 3456, 3538, 4031, 4373, 4767, 5746, 6139, 7536, 7719, 7800, 7901, 7951, 8156, 8386, 8565, 9019, 9160, 9922, 9278]\n",
    "\n",
    "dataframe_dict = dict()\n",
    "for key in agent_keys:\n",
    "    dataframe_dict.update({key : pd.read_csv('/Users/ishilov/Documents/risk_paper/risk_paper/data/df_{}.csv'.format(key))})\n",
    "\n",
    "community_size = len(dataframe_dict)\n",
    "\n",
    "for key in agent_keys:\n",
    "    cond_min = (dataframe_dict[key]['demand'].quantile(0.001) <= dataframe_dict[key]['demand'])\n",
    "    cond_max = (dataframe_dict[key]['demand'] <= dataframe_dict[key]['demand'].quantile(0.999))\n",
    "    dataframe_dict[key] = dataframe_dict[key][cond_min & cond_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_params(sample_size, community_size):\n",
    "    res = {}\n",
    "\n",
    "    for sample in range(sample_size):\n",
    "        A_tilde = [random.uniform(0,1) for i in range(community_size)]\n",
    "        B_tilde = [random.uniform(0,1) for i in range(community_size)]\n",
    "\n",
    "        a = [random.uniform(0,1) for i in range(community_size)]\n",
    "        b = [random.uniform(0,1) for i in range(community_size)]\n",
    "        d = [random.uniform(0,1) for i in range(community_size)]\n",
    "\n",
    "        #d_target = [[random.uniform(0,8) for j in range(len(probabilities))] for i in range(community_size)]\n",
    "        #g_res = [[random.uniform(0,3) for j in range(len(probabilities))] for i in range(community_size)]\n",
    "\n",
    "        #g_res = np.array(g_res)\n",
    "        #d_target = np.array(d_target)\n",
    "\n",
    "        risk_aversion = [random.uniform(0,1) for i in range(community_size)]\n",
    "\n",
    "        res.update({sample : {'A_tilde' : A_tilde,\n",
    "                        'B_tilde' : B_tilde,\n",
    "                        'a' : a,\n",
    "                        'b' : b,\n",
    "                        'd' : d,\n",
    "                        'risk_aversion' : risk_aversion}})\n",
    "\n",
    "    res_reformed = {(i, key) : res[i][key] for i in range(sample_size) for key in res[0].keys()}\n",
    "    mindx = pd.MultiIndex.from_tuples(res_reformed.keys())\n",
    "    df = pd.DataFrame(list(res_reformed.values()), index = mindx)\n",
    "    df.to_csv(f'../data/param_{sample_size}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_adj_matrix(matrix_path):\n",
    "    res = []\n",
    "    with open(matrix_path) as file:\n",
    "        for s in file:\n",
    "            string = ''.join(s.strip().strip(',').split(', '))\n",
    "            lst_temp = [int(sym) for sym in string]\n",
    "\n",
    "            res.append(lst_temp)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_new_params(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_param(sample_size):\n",
    "    df_param = pd.read_csv(f'../data/param_{sample_size}.csv')\n",
    "    df_param.rename({'Unnamed: 0' : 'Sample', 'Unnamed: 1' : 'Parameter'}, axis=1, inplace= True)\n",
    "    df_param.set_index(['Sample', 'Parameter'], inplace=True)\n",
    "\n",
    "    return df_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_build(sample_size, agent_keys):\n",
    "    res = {}\n",
    "    \n",
    "    for key in agent_keys:\n",
    "        #chunks_demand = int(demand_dict[key][0].size / sample_size)\n",
    "        #chunks_generation = int(solar_dict[key][0].size / sample_size)\n",
    "\n",
    "\n",
    "        #probas_demand = [np.trapz(demand_dict[key][1][i * sample_size : (i + 1) * sample_size],\n",
    "        #                            demand_dict[key][0][i * sample_size : (i + 1) * sample_size])\n",
    "        #                            for i in range(chunks_demand)]\n",
    "\n",
    "        #probas_generation = [np.trapz(generation_dict[key][1][i * sample_size : (i + 1) * sample_size],\n",
    "        #                            generation_dict[key][0][i * sample_size : (i + 1) * sample_size])\n",
    "        #                            for i in range(chunks_generation)]\n",
    "\n",
    "        #res_demand = plt.hist(np.random.choice(demand_dict[key][0], size = sample_size, p = probabilities), bins = int(sample_size / 2))\n",
    "        #probas_update = res_demand[0] / res_demand[0].sum() if key == 661 else res[661]['probabilities']\n",
    "        #res.update({key : \n",
    "        #            {'values' : res_demand[1], \n",
    "        #            'probabilities' : probas_update}})\n",
    "\n",
    "        hist_demand = plt.hist(dataframe_dict[key]['demand'], bins = sample_size)\n",
    "        probas_demand, values_demand = hist_demand[0], hist_demand[1]\n",
    "        probas_demand = probas_demand / probas_demand.sum()\n",
    "\n",
    "        if 'solar' in dataframe_dict[key].columns:\n",
    "            hist_solar = plt.hist(dataframe_dict[key][dataframe_dict[key]['solar'] >= 0]['solar'], bins = sample_size)\n",
    "            probas_solar , values_solar  = hist_solar[0], hist_solar[1]\n",
    "            probas_solar = probas_demand / probas_demand.sum()\n",
    "\n",
    "        res.update({key : \n",
    "                    {'probas_demand' : probas_demand,\n",
    "                    'values_demand' : values_demand,\n",
    "                    'probas_solar' : probas_solar,\n",
    "                    'values_solar' : values_solar}})\n",
    "\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_sampling(sample_size, agent_keys, main_key = 661):\n",
    "    distribution = distribution_build(sample_size, agent_keys)\n",
    "\n",
    "    probabilities = distribution[main_key]['probas_demand']\n",
    "\n",
    "    d_target = []\n",
    "    g_res = []\n",
    "    for key in agent_keys:\n",
    "        d_target.append(distribution[key]['values_demand'][:-1])\n",
    "        g_res.append(distribution[key]['values_solar'][:-1])\n",
    "\n",
    "    return probabilities, d_target, g_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_input(df, index = 0):\n",
    "    A_tilde = list(df.loc[index].loc['A_tilde'])\n",
    "    B_tilde = list(df.loc[index].loc['B_tilde'])\n",
    "    a = list(df.loc[index].loc['a'])\n",
    "    b = list(df.loc[index].loc['b'])\n",
    "    d = list(df.loc[index].loc['d'])\n",
    "    risk_aversion = list(df.loc[index].loc['risk_aversion'])\n",
    "\n",
    "    for i, RA in enumerate(risk_aversion):\n",
    "        if RA > 0.95:\n",
    "            risk_aversion[i] = risk_aversion[i] - 0.1\n",
    "\n",
    "    return A_tilde, B_tilde, a, b, d, risk_aversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_amount = 2000\n",
    "\n",
    "#generate_new_params(param_amount,community_size=community_size)\n",
    "df_param = read_df_param(param_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_csv(scenario_amount, probabilities, d_target, g_res):\n",
    "    pd.DataFrame(d_target).to_csv(f'../data/df_d_target_{scenario_amount}.csv')\n",
    "    pd.DataFrame(g_res).to_csv(f'../data/df_g_res_{scenario_amount}.csv')\n",
    "    pd.DataFrame(probabilities).to_csv(f'../data/df_probabilities_{scenario_amount}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_csv(scenario_amount):\n",
    "    df_d_target = pd.read_csv(f'../data/df_d_target_{scenario_amount}.csv').drop('Unnamed: 0', axis = 1)\n",
    "    df_g_res = pd.read_csv(f'../data/df_g_res_{scenario_amount}.csv').drop('Unnamed: 0', axis = 1)\n",
    "    df_probabilities =pd.read_csv('../data/df_probabilities_100.csv').drop('Unnamed: 0', axis = 1)\n",
    "    \n",
    "    probabilities = df_probabilities.values.squeeze()\n",
    "\n",
    "\n",
    "    d_target = []\n",
    "    for _, row in df_d_target.iterrows():\n",
    "        d_target.append(row.values)\n",
    "\n",
    "    g_res = []\n",
    "    for _, row in df_g_res.iterrows():\n",
    "        g_res.append(row.values)\n",
    "\n",
    "    return probabilities, d_target, g_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities, d_target, g_res = sample_from_csv(100)\n",
    "#A_tilde, B_tilde, a, b, d, risk_aversion = param_input(df_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agents_list_optimistic_total(A_tilde, B_tilde, a, b, d, risk_aversion, probabilities, connection_matrix, d_target, g_res):\n",
    "    agents = []\n",
    "    StackelbergPlayer.community_size = community_size\n",
    "    StackelbergPlayer.probabilities = probabilities\n",
    "\n",
    "    epsilon = 0.001\n",
    "    alpha = [[proba/(1 - min(risk_aversion)) for proba in probabilities] for i in range(community_size)]\n",
    "    #alpha = [[0.2 for proba in probabilities] for i in range(community_size)]\n",
    "    gamma = [proba/(1 - min(risk_aversion))for proba in probabilities]\n",
    "\n",
    "    j_max = [10 for i in range(community_size)]\n",
    "\n",
    "    for i in range(community_size):\n",
    "        agent = StackelbergPlayer(i, d_target[i], g_res[i], a[i], b[i], d[i], \n",
    "                    A_tilde[i], B_tilde[i], D_min[i], D_max[i], \n",
    "                    G_min[i], G_max[i], risk_aversion[i], Kappa[i], Cost[i], connection_matrix[i],\n",
    "                    probabilities = probabilities,\n",
    "                    alpha = alpha[i], \n",
    "                    gamma = gamma, \n",
    "                    insurance_bound=j_max[i])\n",
    "        \n",
    "        agents.append(agent)\n",
    "\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vars_to_dict(model, list_vars):\n",
    "    var_names = []\n",
    "    \n",
    "    for var in model.getVars():\n",
    "        var_names.append(var.VarName)\n",
    "\n",
    "    dict_res = {}\n",
    "    for name, var in zip(var_names, list_vars):\n",
    "        dict_res.update({name : var})\n",
    "\n",
    "    return dict_res \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gurobi_experiment(df_param, index, err_track, connection_matrix, probabilities, d_target, g_res, solution_type, verbosity = 0):\n",
    "    A_tilde, B_tilde, a, b, d, risk_aversion = param_input(df_param, index = index)\n",
    "\n",
    "    if solution_type == 'without_IC':\n",
    "        agents = agents_list_optimistic_total(A_tilde, B_tilde, a, b, d, risk_aversion, probabilities, connection_matrix, d_target, g_res)\n",
    "\n",
    "    model_1 = gp.Model()\n",
    "    setup = GurobiSolution(agents=agents,\n",
    "                    model = model_1,\n",
    "                    solution_type=solution_type)\n",
    "\n",
    "    model_1.setParam('OutputFlag', verbosity)\n",
    "\n",
    "    setup.build_model()\n",
    "\n",
    "    model_1.display()\n",
    "\n",
    "    try:\n",
    "        model_1.optimize()\n",
    "\n",
    "        list_vars = model_1.X\n",
    "        dict_vars = vars_to_dict(model_1, list_vars)\n",
    "        objective_val = model_1.getObjective().getValue()\n",
    "        \n",
    "        \n",
    "        \n",
    "    except:\n",
    "        err_track.append(index)\n",
    "        list_vars = ['err']\n",
    "        objective_val = 'err'\n",
    "        dict_vars = vars_to_dict(model_1, list_vars)\n",
    "\n",
    "    return dict_vars, objective_val, model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_min = [0 for i in range(community_size)]\n",
    "D_max = [dataframe_dict[i].demand.max() for i in dataframe_dict.keys()]\n",
    "\n",
    "G_min = [0 for i in range(community_size)]\n",
    "G_max = [dataframe_dict[i].grid.max() for i in dataframe_dict.keys()]\n",
    "\n",
    "Kappa = [[10 if i!=j else 0 for i in range(community_size)] for j in range(community_size)]\n",
    "\n",
    "Cost = [[1 for i in range(community_size)] for j in range(community_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_matrix_2 = text_to_adj_matrix('../matrices/matrix_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(probabilities, connection_matrix, d_target, g_res,\n",
    "                df_param, solution_type='without_IC', verbosity = 0):\n",
    "\n",
    "    total_results= {}\n",
    "    err_track = []\n",
    "    for index in tqdm_notebook(df_param.index.levels[0]):\n",
    "        vars, objective, model = gurobi_experiment(df_param, index, err_track, connection_matrix, probabilities, d_target, g_res, solution_type, verbosity)\n",
    "        total_results.update({index: {'vars' : vars,\n",
    "                                    'objective' : objective}})\n",
    "\n",
    "\n",
    "    results_vars = pd.DataFrame(data = [list(total_results[0]['vars'].values())],\n",
    "                                        columns=total_results[0]['vars'].keys())\n",
    "\n",
    "    results_vars['objective'] = total_results[0]['objective']\n",
    "\n",
    "    for i in tqdm_notebook(range(1, len(total_results))):\n",
    "        df_temp = pd.DataFrame(data = [list(total_results[i]['vars'].values())],\n",
    "                                            columns=total_results[i]['vars'].keys())\n",
    "                                \n",
    "        df_temp['objective'] = total_results[i]['objective']\n",
    "\n",
    "        results_vars = pd.concat([results_vars, df_temp], ignore_index=True)\n",
    "\n",
    "    return total_results, results_vars, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/gq_0lybs5k55th2hc4jstmyw00m7tv/T/ipykernel_7884/151759034.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index in tqdm_notebook(df_param.index.levels[0][df_param.index.levels[0]<1]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1e0cc5cc34426bb16da1a56de7a979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "\n",
      "--------------------------------------------\n",
      "Warning: your license will expire in 1 days\n",
      "--------------------------------------------\n",
      "\n",
      "Academic license - for non-commercial use only - expires 2022-04-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/gq_0lybs5k55th2hc4jstmyw00m7tv/T/ipykernel_7884/151759034.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(1, len(total_results))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4ec2af7a234146aca17a92094a8ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_without_IC, df_without_IC, model_without_IC = experiment(probabilities, connection_matrix_2, d_target, g_res,\n",
    "            df_param, solution_type='without_IC', verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D_0_0</th>\n",
       "      <th>D_0_1</th>\n",
       "      <th>D_0_2</th>\n",
       "      <th>D_0_3</th>\n",
       "      <th>D_0_4</th>\n",
       "      <th>D_0_5</th>\n",
       "      <th>D_0_6</th>\n",
       "      <th>D_0_7</th>\n",
       "      <th>D_0_8</th>\n",
       "      <th>D_0_9</th>\n",
       "      <th>...</th>\n",
       "      <th>u_24_91</th>\n",
       "      <th>u_24_92</th>\n",
       "      <th>u_24_93</th>\n",
       "      <th>u_24_94</th>\n",
       "      <th>u_24_95</th>\n",
       "      <th>u_24_96</th>\n",
       "      <th>u_24_97</th>\n",
       "      <th>u_24_98</th>\n",
       "      <th>u_24_99</th>\n",
       "      <th>objective</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.979406e-10</td>\n",
       "      <td>0.030437</td>\n",
       "      <td>0.094768</td>\n",
       "      <td>0.158483</td>\n",
       "      <td>0.2215</td>\n",
       "      <td>0.284118</td>\n",
       "      <td>0.346916</td>\n",
       "      <td>0.409206</td>\n",
       "      <td>0.47113</td>\n",
       "      <td>0.533054</td>\n",
       "      <td>...</td>\n",
       "      <td>2.932006e-09</td>\n",
       "      <td>2.065826e-09</td>\n",
       "      <td>3.285718e-09</td>\n",
       "      <td>2.068067e-09</td>\n",
       "      <td>3.288983e-09</td>\n",
       "      <td>1.808489e-09</td>\n",
       "      <td>2.941596e-09</td>\n",
       "      <td>4.344084e-09</td>\n",
       "      <td>4.346136e-09</td>\n",
       "      <td>0.833411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 17326 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          D_0_0     D_0_1     D_0_2     D_0_3   D_0_4     D_0_5     D_0_6  \\\n",
       "0  5.979406e-10  0.030437  0.094768  0.158483  0.2215  0.284118  0.346916   \n",
       "\n",
       "      D_0_7    D_0_8     D_0_9  ...       u_24_91       u_24_92       u_24_93  \\\n",
       "0  0.409206  0.47113  0.533054  ...  2.932006e-09  2.065826e-09  3.285718e-09   \n",
       "\n",
       "        u_24_94       u_24_95       u_24_96       u_24_97       u_24_98  \\\n",
       "0  2.068067e-09  3.288983e-09  1.808489e-09  2.941596e-09  4.344084e-09   \n",
       "\n",
       "        u_24_99  objective  \n",
       "0  4.346136e-09   0.833411  \n",
       "\n",
       "[1 rows x 17326 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_without_IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_IC.to_csv(f'../data/without_IC_results_{len(df_without_IC)}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_utility_from_df_row(df_row, agents):\n",
    "    res = []\n",
    "    for agent in agents:\n",
    "        eta_idx = f'eta_{agent.id}'\n",
    "        agent_obj = df_row[eta_idx]\n",
    "\n",
    "        for proba in agent.probabilities_ind:\n",
    "            u_idx = f'u_{agent.id}_{proba}'\n",
    "            w_idx = f'W_{agent.id}_{proba}'\n",
    "            j_idx = f'J_{agent.id}_{proba}'\n",
    "\n",
    "            agent_obj += (agent.alpha[proba] * df_row[j_idx]\n",
    "                        + agent.gamma[proba] * df_row[w_idx]\n",
    "                        + agent.probabilities[proba] / (1 - agent.risk_aversion) * df_row[u_idx])\n",
    "\n",
    "        res.append(agent_obj)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IC_utility_from_df_row(df_row, agents):\n",
    "    res = 0\n",
    "\n",
    "    for agent in agents:\n",
    "        for proba in agent.probabilities_ind:\n",
    "            j_idx = f'J_{agent.id}_{proba}'\n",
    "\n",
    "            res += - agent.alpha[proba] * df_row[j_idx] + agent.probabilities[proba] * df_row[j_idx]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_objectives = []\n",
    "for index, row in df_without_IC.iterrows():\n",
    "    A_tilde, B_tilde, a, b, d, risk_aversion = param_input(df_param, index = index)\n",
    "    agents = agents_list_optimistic_total(A_tilde, B_tilde, a, b, d, risk_aversion, probabilities, connection_matrix_2, d_target, g_res)\n",
    "    agent_objectives.append(agent_utility_from_df_row(row, agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_param = df_param.to_dict()\n",
    "for i in range(community_size):\n",
    "    for trial_idx, trial in enumerate(agent_objectives):\n",
    "        dict_df_param[f'{i}'].update({(trial_idx, 'objective') : trial[i]})\n",
    "\n",
    "df_param = pd.DataFrame(dict_df_param).sort_index(level=0)\n",
    "df_param.to_csv(f'../data/without_IC_params_{2000}_with_obj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.22277904918520872,\n",
       "  0.4416655418112445,\n",
       "  -0.3420582377334909,\n",
       "  0.5107292360140728,\n",
       "  -0.4280968017542822,\n",
       "  0.20718199927799683,\n",
       "  -0.7986964938805798,\n",
       "  -0.043486866537898225,\n",
       "  0.17218716238096046,\n",
       "  -0.0781053349714372,\n",
       "  1.1834175012835197,\n",
       "  0.4600742945087344,\n",
       "  0.13291822880153573,\n",
       "  0.4751315480720912,\n",
       "  0.4124036536360921,\n",
       "  -0.009026129514583283,\n",
       "  0.058381443357569354,\n",
       "  0.1537682805781731,\n",
       "  0.45253768340046713,\n",
       "  -0.2353208172716938,\n",
       "  0.6913183774527831,\n",
       "  0.11273708439037083,\n",
       "  -0.8564327739161616,\n",
       "  0.26958008071127765,\n",
       "  -2.3321763404975333]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_objectives"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc51f7b0a90f6051b3373596df13638ac01198d8ab4bced12ffaa78c40a2d902"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
